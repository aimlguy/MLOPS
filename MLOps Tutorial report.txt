Rashtreeya Sikshana Samithi Trust
R. V. COLLEGE OF ENGINEERING
[Autonomous Institution Affiliated to VTU, Belagavi] 
Department of Artificial Intelligence and Machine Learning
 Bengaluru-560059




  



MLOps Tutorial Report
Subject Code: AI254TA


Forecasting No Show in Healthcare


V SEMESTER B.E.


[Autonomous Scheme 2022]


2024-2025

Submitted by  


TABLE OF CONTENTS


1. Problem Statement and Objectives        1
1.1 Defining Problem Statement and Objectives        1
1.2 Project Scope and Evaluation Metrics        1
1.3 Tools and Environment Setup        2
1.4 Literature Review        3
2. Understanding MLOps Risks and Requirement Analysis        9
2.1 Understanding MLOps Risks        9
2.2 Requirement Analysis        9
3. Data Understanding, Feature Engineering, and Governance        11
3.1 Data Collection and Exploration Techniques        11
3.2 Data Profiling, Cleaning, and Preprocessing        11
3.3 Feature Extraction and Feature Selection        12
3.4 Model Design Approaches        12
3.5 Governance and Data Version Control        12
4.1 Model Building        14
4.2 Training, Validation, and Testing        14
4.3 Evaluation Metrics and Visualization        14
4.4 Experiment Tracking        15
4.5 Interpreting Model Results and Error Analysis        16
5. Model Analysis and Refinement        17
5.1 Revisiting Model Performance        17
5.2 Hyperparameter Tuning and Optimization        17
5.3 Interpretability        17
5.4 Model Versioning and Registry        18
5.5 Documentation of Model Updates        19
6. Implementing CI/CD Pipeline        20
6.1 Overview of CI/CD for ML        20
6.2 Tools Setup        20
6.3 Automating Model Training and Testing        20
6.4 Building and Running a Deployment Pipeline        21
7. Monitoring and Tracking Model Lifecycle        22
7.1 Model Monitoring and KPIs        22
7.2 Tools used        22
7.3 Detecting Concept Drift and Data Drift        23
7.4 Model Retraining and Redeployment        23
7.5 Logging, Alerting, and Dashboard Setup        24
8. Performance Evaluation and Governance        25
8.1 Post-Deployment Performance Review        25
8.2 Continuous Improvement via Feedback Loops        25
8.3 Governance and Ethical AI Practices        25
8.4 Review Checklist        26


1. Problem Statement and Objectives
1.1 Defining Problem Statement and Objectives
In the realm of healthcare operations, patient no-shows pose a multifaceted challenge that disrupts efficient resource allocation, escalates operational costs, and compromises patient care continuity. A no-show occurs when a patient fails to attend a scheduled medical appointment without prior cancellation, leading to idle clinic slots, prolonged waiting lists for other patients, and potential revenue losses for healthcare providers. This issue is particularly acute in public healthcare systems, where resources are often constrained, and missed appointments can exacerbate health disparities among vulnerable populations.
The core problem this project addresses is the development of a predictive model to forecast patient no-shows using historical appointment data. By leveraging machine learning techniques, the model aims to identify high-risk appointments in advance, enabling targeted interventions like automated reminders, rescheduling prompts, or overbooking strategies. This proactive approach not only mitigates the immediate effects of no-shows but also contributes to broader systemic improvements in healthcare delivery.
Primary objectives include:
* Reduce No-Show Rates: Achieve a 15% reduction in no-show rates within 6 months of deployment by prioritizing interventions for high-risk patients.
* Improve Resource Utilization: Increase appointment slot utilization by 10% through accurate predictions, minimizing wasted staff time and clinic capacity.
* Enhance Patient Outcomes: Ensure timely follow-ups for patients with chronic conditions (e.g., hypertension, diabetes) by reducing missed appointments by 20% for these groups.
* Ensure Fairness: Maintain equitable model performance across demographic groups (e.g., gender, age, socioeconomic status) to comply with ethical and legal standards
1.2 Project Scope and Evaluation Metrics
The project scope encompasses the full MLOps lifecycle for a no-show prediction model, from data ingestion and feature engineering to model training, evaluation, deployment considerations, and ongoing monitoring. Key inclusions are:
* Data Handling: Utilization of the publicly available "Medical Appointment No Shows" dataset from Kaggle, focusing on preprocessing, feature extraction, and versioning to ensure traceability.
* Model Development: Implementation and comparison of three algorithms i.e. Logistic Regression, Random Forest, and XGBoost, with emphasis on handling class imbalance, hyperparameter tuning, and interpretability.
* MLOps Integration: Incorporation of tools for data versioning, experiment tracking, and risk management to support reproducibility and scalability.
* Ethical Considerations: Analysis of biases, fairness metrics, and mitigation strategies to align with healthcare ethics.
Evaluation metrics are selected to balance predictive performance, operational utility, and ethical integrity. Primary metrics include:
* Accuracy: Measures the overall correctness of predictions, with a target of >80% to ensure the model is reliable for clinical use without excessive false alarms.
* AUC-ROC (Area Under the Receiver Operating Characteristic Curve): Assesses the model's ability to distinguish between no-show and attendance classes, targeting >0.85 to achieve a strong balance between sensitivity (detecting true no-shows) and specificity (avoiding unnecessary interventions), in line with benchmarks from similar Kaggle competitions and healthcare ML studies.
* Precision, Recall, and F1-Score: Precision evaluates the proportion of predicted no-shows that are actual no-shows (minimizing false positives to avoid overburdening staff); Recall focuses on capturing most actual no-shows (high sensitivity for patient outcomes); F1-Score provides a harmonic mean for imbalanced classes.
* Operational Metrics: Inference latency (<500ms per prediction) for seamless integration into real-time systems, and cost-efficiency in compute resources to stay within budget constraints.
1.3 Tools and Environment Setup
The project employed a comprehensive, production-oriented MLOps stack built primarily on open-source tools to ensure reproducibility, scalability, auditability, and cost efficiency throughout the entire machine learning lifecycle.
* Development and Prototyping: Initial exploratory data analysis (EDA), feature engineering, and model prototyping were conducted in Kaggle notebooks, leveraging their free-tier GPU/CPU resources and direct integration with the dataset for rapid iteration without additional infrastructure costs.
* Version Control: GitHub served as the central repository for code, documentation, configuration files, and pipeline definitions, enabling collaborative development and versioned history.
* Data Versioning: DVC (Data Version Control) was used to track and version large datasets (raw CSV, cleaned, and feature-engineered Parquet files). This provided Git-like traceability, lightweight pointers in the repository, reproducible checkpoints, and seamless rollbacks via dvc checkout.
* Experiment Tracking and Model Management: MLflow was the core platform for logging hyperparameters, metrics (AUC, accuracy, precision, recall, F1), artifacts (pickled models, SHAP plots, classification reports), and full experiment metadata. Its UI facilitated side-by-side run comparisons and supported the Model Registry for staging and promoting production candidates.
* Core Modeling Libraries: Python was the primary language, with:
   * Pandas and NumPy for data manipulation and exploration.
   * Scikit-learn for baseline models (Logistic Regression, Random Forest), preprocessing (LabelEncoder, StandardScaler), hyperparameter tuning (GridSearchCV), and evaluation.
   * XGBoost for the high-performance gradient boosting model.
   * SHAP for global and local model interpretability and fairness analysis.
* CI/CD Automation: GitHub Actions provided continuous integration, automatically triggering tests, linting, and pipeline validation on code pushes and pull requests to maintain code quality and reliability.
* Containerization and Serving: Docker was used to package the trained model and inference code into reproducible containers, ensuring consistency across development, testing, and production environments. FastAPI wrapped the model to create a lightweight, high-performance prediction API.
* Orchestration and Deployment: Apache Airflow was used for orchestration of the different MLOps processes in the pipeline. Kubernetes was employed for scalable deployment, managing container orchestration, auto-scaling, and rolling updates in a production-like setting.
* Production Monitoring and Drift Detection: Evidently AI, integrated with Prometheus for metric collection and Grafana for visualization, enabled real-time monitoring of model performance, data drift, target drift, inference latency, and prediction distribution. Custom dashboards and alerts facilitated proactive detection of degradation and supported ongoing model maintenance.
This integrated stack—combining free-tier prototyping tools with robust open-source MLOps components, delivered end-to-end traceability, automated workflows, and production-grade observability while remaining cost-effective and aligned with best practices for responsible healthcare AI deployment.
1.4 Literature Review
Research on predicting patient no-shows using machine learning has proliferated over the past few decades, driven by the persistent 30 - 40% no-show rates in outpatient settings and their associated costs (estimated at hundreds of dollars per missed appointment). Studies predominantly utilize real-world datasets similar to the Kaggle "Medical Appointment No Shows" collection, applying algorithms ranging from logistic regression and decision trees to ensembles like Random Forest and gradient boosting (e.g., XGBoost), often reaching the higher end with advanced feature engineering and imbalance handling. Applications span general primary care, pediatrics, specialty clinics, telemedicine etc. Recent works emphasize interpretability (via SHAP or feature importance), fairness concerns around socioeconomic biases, real-time integration with EHRs, and downstream interventions such as targeted reminders, overbooking, or personalized scheduling, demonstrating potential reductions in no-show rates from 10% to over 50%. Comprehensive reviews highlight the superiority of ensemble methods for imbalanced data while noting persistent gaps in model interpretability, ethical bias mitigation, generalizability and robust handling of data quality issues. The following summarizes 25 key studies from 2005–2025.


1. K. M. Toffaha, M. C. E. Simsekler, M. A. Omar, and I. ElKebbi, "Predicting patient no-shows using machine learning: A comprehensive review and future research agenda," Healthcare Analytics, vol. 5, p. 100334, Jun. 2025.
Summary: This paper reviews 52 studies from 2010-2025 on ML methods for predicting outpatient no-shows, noting logistic regression's prevalence but ensemble models' superiority in handling imbalanced data like the Kaggle dataset. It discusses performance metrics such as AUC (0.75-0.95) and explores how ML can optimize healthcare resources by enabling interventions like reminders. The review also outlines future agendas, including ethical ML implementation and transfer learning for cross-setting adaptability. 
Gaps and Opportunities: Limited focus on data quality and model interpretability hinders clinical adoption; opportunities include integrating external factors like weather into XGBoost models for real-time predictions and addressing ethical biases to fill gaps in underserved regions.
2. J. Chen, I. H. Goldstein, W.-C. Lin, M. F. Chiang, and M. R. Hribar, "Application of machine learning to predict patient no-shows in an academic pediatric ophthalmology clinic," AMIA Annu. Symp. Proc., vol. 2020, pp. 293-302, 2020.
Summary: Utilizing EHR data, this study applies ML algorithms including XGBoost to forecast no-shows in pediatric ophthalmology, achieving AUC scores up to 0.90 for follow-ups. It highlights features like patient history and distance as key predictors, demonstrating ML's potential to enhance scheduling efficiency. The work suggests targeted interventions based on predictions to reduce resource waste in specialized care. 
Gaps and Opportunities: Reliance on single-institution data limits generalizability; opportunities lie in expanding to multi-site Kaggle-like datasets with XGBoost for broader pediatric applications and incorporating socioeconomic variables to bridge rural-urban disparities.
3. D. Liu, W. Y. Shin, E. Sprecher, K. Conroy, O. Santiago, G. Wachtel, and M. Santillana, "Machine learning approaches to predicting no-shows in pediatric medical appointment," npj Digit. Med., vol. 5, no. 50, Apr. 2022.
Summary: This research introduces interpretable deep learning for no-show prediction in a pediatric hospital with a 20% no-show rate, using features like weather and patient records. It improves accuracy over traditional methods and discusses interventions like off-peak scheduling for high-risk patients. The approach leverages datasets similar to Kaggle to handle missing data and class imbalance. 
Gaps and Opportunities: Data imputation challenges persist in incomplete records; opportunities include hybrid XGBoost-deep learning models for causal analysis and real-time integration with EHRs to reduce no-shows in resource-limited clinics.
4. G. Fan, Z. Deng, Q. Ye, and B. Wang, "Machine learning-based prediction models for patients no-show in online outpatient appointments," Data Sci. Manag., vol. 1, no. 2, pp. 91-99, Jun. 2021.
Summary: Focusing on online appointments, this study employs algorithms like random forest and bagging to predict no-shows with AUC up to 0.990, using multi-source data including demographics. It demonstrates ML's role in policy optimization to minimize resource waste. The models build on Kaggle-inspired data for handling diverse predictors. 
Gaps and Opportunities: Single-source data limitations affect robustness; opportunities involve XGBoost for multi-regional transfer and socioeconomic augmentation to optimize overbooking in online healthcare systems.
5. J. J. Joseph, S. Senith, A. A. Kirubaraj, and S. R. JinoRamson, "Machine learning-based prediction of no-show telemedicine appointments," Preprint, 2025. 
Summary: This paper explores ML ensembles for predicting no-shows in Peruvian telemedicine, emphasizing cultural and access features. It highlights high performance in underserved areas, using approaches akin to XGBoost for real-time adjustments. The work addresses how ML can enhance equity in remote care settings. 
Gaps and Opportunities: Cultural factors are underexplored; opportunities include integrating Kaggle data with local variables for XGBoost models to scale predictions in global telemedicine, filling gaps in low-resource environments.
6. A. Z. Dag, "Predicting pediatric diagnostic imaging patient no-show and long wait-times," Preprint, 2025. 
Summary: Surveying ML methods like random forest for imaging no-shows, this study integrates wait-time predictions to improve scheduling. It achieves higher AUC with RF, focusing on pediatric data similar to Kaggle. The approach aims to combine predictions with overbooking for efficiency gains. 
Gaps and Opportunities: Limited to imaging; opportunities in XGBoost for multi-specialty fusion and temporal data inclusion to address dynamic wait-time gaps.
7. W.-J. Tuan, W.-J. Tu, Q. Zeng, and X. B. Ling, "Predicting missed appointments in primary care: A personalized machine learning approach," Ann. Fam. Med., vol. 23, no. 4, pp. 294-299, Jul.-Aug. 2025.
Summary: Using gradient boosting and neural networks, this work predicts missed primary care appointments with multiclass models, incorporating demographics and history. It enables nuanced interventions to optimize adherence. The study leverages Kaggle-like data for probabilistic scheduling.
Gaps and Opportunities: Binary vs. multiclass modeling discrepancies; opportunities for XGBoost in personalized reminders and socioeconomic integration to reduce no-shows by 10-20%.
8. Y. M. AlSerkal, N. M. Ibrahim, A. S. Alsereidi, M. Ibrahim, S. Kurakula, S. A. Naqvi, Y. Khan, and N. P. Oottumadathil, "Real-time analytics and AI for managing no-show appointments in primary health care in the United Arab Emirates: Before-and-after study," JMIR Form. Res., vol. 9, p. e64936, Jan. 2025.
Summary: Integrating AI ensembles with EHRs, this study reduces no-shows by 50.7% in primary care, using real-time data for adjustments. It demonstrates ML's impact on scalability and resource allocation. The approach uses Kaggle-inspired features for dynamic interventions. 
Gaps and Opportunities: Scalability in rural areas; opportunities in XGBoost-API integration for global adoption and causal modeling to address socioeconomic gaps.
9. A. R. Shour, G. L. Jones, R. Anguzu, S. Doi, and A. Onitilo, "Development of an evidence-based model for predicting patient, provider, and appointment factors that influence no-shows in a rural healthcare system," BMC Health Serv. Res., vol. 23, p. 989, Sep. 2023.
Summary: Employing XGBoost on rural data, this model predicts no-shows with strong performance, focusing on patient-provider factors. It supports dynamic overbooking to enhance access in underserved areas. The study uses Kaggle-like attributes for evidence-based scheduling. 
Gaps and Opportunities: Socioeconomic inclusion lacking; opportunities in XGBoost with external data for causal insights and multi-organization transfer to bridge rural gaps.
10. M. U. Ahmad, A. Zhang, and R. Mhaskar, "Machine learning predictions of no-show appointments in a primary care clinic," Int. J. Healthc. Manag., vol. 14, no. 3, pp. 829-836, 2021. 
Summary: This paper develops probabilistic ML models for no-show risk in primary care, using patient estimates for resource allocation. It proposes solutions like overbooking based on predictions. The work draws on Kaggle-inspired data for practical scheduling improvements. 
Gaps and Opportunities: Primary vs. specialty care differences; opportunities in XGBoost for probabilistic enhancements and disease-specific adaptations.
   11. A. Alshammari, R. Almalki, and R. Alshammari, "Developing a predictive model of predicting appointment no-show of diabetic patients," J. Adv. Inf. Technol., vol. 12, no. 3, pp. 234-239, 2021. 
Summary: Focusing on diabetic patients, this study uses ML ensembles to predict no-shows, building on literature for disease-specific risks. It highlights how ML can reduce complications through better attendance. The models incorporate health conditions akin to Kaggle features. 
Gaps and Opportunities: Disease-specific focus limits breadth; opportunities in XGBoost for integrating with EHRs and expanding to other chronic conditions.
      12. L. H. A. Salazar, W. D. Parreira, A. M. d. R. Fernandes, and V. R. Q. Leithardt, "No-show in medical appointments with machine learning techniques: A systematic literature review," Information, vol. 13, no. 11, p. 507, Nov. 2022.
Summary: Synthesizing 24 papers, this SLR shows random forest's dominance in no-show prediction with 23% average rates. It emphasizes ML for understanding behavior and scheduling. The review uses Kaggle data for interpretability insights. 
Gaps and Opportunities: Interpretability issues; opportunities in XGBoost feature engineering and causal analysis for global applicability.
      13. F. S. Fogliatto, G. J. C. da Silveira, and M. J. Anzanello, "Decision analysis framework for predicting no-shows to appointments using machine learning algorithms," BMC Health Serv. Res., vol. 24, p. 37, Jan. 2024.
Summary: Proposing ML frameworks like symbolic regression, this study predicts no-shows for personalized scheduling in Brazilian hospitals. It validates with datasets showing 6-19% rates. The approach optimizes overbooking using Kaggle-like characteristics. 
Gaps and Opportunities: Static methods prevail; opportunities in XGBoost for dynamic strategies and imbalanced data handling.
      14. A. Alshammari et al., "Prediction of outpatient no-show appointments using machine learning algorithms for pediatric patients in Saudi Arabia," IEEE Access, 2024. 
Summary: Applying gradient boosting and neural networks to pediatric data, this achieves 98% precision in no-show prediction. It uses attributes similar to Kaggle for multi-organization insights. The work enhances efficiency in regional settings. 
Gaps and Opportunities: Regional context limits; opportunities in XGBoost for cross-cultural models and socioeconomic enhancements.
      15. A. F. A. Hamdan and A. A. Bakar, "Machine learning predictions on outpatient no-show appointments in a Malaysia major tertiary hospital," Malays. J. Med. Sci., vol. 30, no. 5, pp. 169-180, Oct. 2023.
Summary: Proposing decision trees for no-show prediction in Malaysian hospitals, this evaluates accuracy with socioeconomic data. It improves models using Kaggle-like demographics. The study addresses local gaps in healthcare efficiency. 
Gaps and Opportunities: Malaysian-specific; opportunities in XGBoost for regional adaptations and multi-hospital data integration.
      16. S. Alshaya, A. McCarren, and A. Al-Rasheed, "Predicting no-show medical appointments using machine learning," J. Med. Syst., 2019. 
Summary: Building binary classification models on Kaggle data, this identifies lead time as a key predictor for reducing no-show consequences. It uses ML for causal exploration. The approach optimizes resource use in general care. 
Gaps and Opportunities: Basic binary focus; opportunities in XGBoost temporal modeling for enhanced predictions.
      17. L. H. A. Salazar et al., "Application of machine learning techniques to predict a patient's no-show in the healthcare sector," Future Internet, vol. 14, no. 1, p. 3, 2022. 
Summary: Exploring ML for no-show causes using demographics and health data, this identifies patterns on Kaggle datasets. It aids in identification and interventions. The work promotes causal ML in healthcare. 
Gaps and Opportunities: Causal depth lacking; opportunities in XGBoost for advanced analysis and policy integration.
      18. M. U. Ahmad, A. Zhang, and R. Mhaskar, "A predictive model for decreasing clinical no-show rates in a primary care setting," Int. J. Healthc. Manag., vol. 14, no. 3, pp. 829-836, 2021.
Summary: Developing ensemble models to reduce no-show rates in primary care, this achieves effective predictions for interventions. It references multiple reviews for best practices. The study uses Kaggle-inspired data for continuity. 
Gaps and Opportunities: Specialty vs. primary discrepancies; opportunities in XGBoost for nuanced modeling.
      19. Y.-L. Huang and D. A. Hanauer, "Time dependent patient no-show predictive modelling development," Int. J. Health Care Qual. Assur., vol. 29, no. 4, pp. 475-488, May 2016.
Summary: Incorporating time-dependent components into ML for no-show prediction, this improves over static models using lead time and history. It supports overbooking policies. The work lays groundwork for temporal ML in clinics. 
Gaps and Opportunities: Early static limitations; opportunities in XGBoost enhancements for modern datasets like Kaggle.
      20. S. Srinivas and H. Salah, "Consultation length and no-show prediction for improving appointment scheduling efficiency at a cardiology clinic: A data analytics approach," Int. J. Med. Inform., vol. 145, p. 104290, Jan. 2021.
Summary: Predicting consultation length and no-shows with ML like DNN, this integrates predictions for cardiology scheduling efficiency. It reduces wait times using patient features. The approach fuses specialties with Kaggle data. 
Gaps and Opportunities: Multi-specialty fusion gaps; opportunities in XGBoost for broader applications.
      21. D. Marbouh, I. Khaleel, K. A. Shanqiti, M. A. Tamimi, M. C. E. Simsekler, S. Ellahham, D. Alibazoglu, and H. Alibazoglu, "Evaluating the impact of patient no-shows on service quality," Risk Manag. Healthc. Policy, vol. 13, pp. 509-517, Jun. 2020.
Summary: Reviewing no-show impacts with case studies, this estimates annual losses like $89,107 at 12% rates. It discusses ML potential for dynamic scheduling to recover costs. The work highlights resource underutilization in radiology. 
Gaps and Opportunities: Economic focus without ML; opportunities in XGBoost for predictive overbooking to mitigate financial gaps.
      22. A. M. Chen, "Socioeconomic and demographic factors predictive of missed appointments in outpatient radiation oncology: an evaluation of access," Front. Health Serv., vol. 3, p. 1288329, Dec. 2023.
Summary: Analyzing missed oncology appointments, this links socioeconomic status and race to no-shows costing $150B/year. It suggests ML like XGBoost for targeting at-risk groups. The study evaluates access barriers in radiation care. 
Gaps and Opportunities: Equity gaps; opportunities in Kaggle-augmented models for causal predictions and interventions.
      23. İ. Toker, K. Ataș, A. Ataş, A. Mayadağlı, Z. Görmezoğlu, İ. Tuncay, and R. Kazancıoğlu, "A solution to reduce the impact of patients’ no-show behavior on hospital operating costs: Artificial intelligence-based appointment system," Healthcare, vol. 12, no. 21, p. 2161, Oct. 2024.
Summary: Developing AI for no-show predictions, this increases attendance by 10% and revenue by $570K in months. It uses patient patterns for system optimization. The approach manages costs at $196 per no-show. 
Gaps and Opportunities: Scalability; opportunities in XGBoost integration for broader economic modeling.
      24. M. Bech, "The economics of non-attendance and the expected effect of charging a fine on non-attendees," Health Policy, vol. 74, no. 2, pp. 181-191, Oct. 2005.
Summary: Exploring non-attendance economics, this analyzes fines' effect on reducing rates but adding admin costs. It discusses social costs and productivity losses. The work suggests ML for cost-effectiveness evaluations. 
Gaps and Opportunities: Empirical fine data lacking; opportunities in XGBoost for predictive fine optimization.
      25. J. R. Fystro and E. Feiring, "Mapping out the arguments for and against patient non-attendance fees in healthcare: an analysis of public consultation documents," J. Med. Ethics, vol. 49, no. 12, pp. 844-849, Dec. 2023.
Summary: Analyzing consultations on non-attendance fees, this maps arguments on equity and costs at 3-4% rates. It debates fines' effectiveness in public systems. The study highlights ML alternatives to fees for behavioral change. 
Gaps and Opportunities: Ethical biases; opportunities in XGBoost for predictive incentives over fines.






2. Understanding MLOps Risks and Requirement Analysis
2.1 Understanding MLOps Risks
In healthcare ML applications like no-show prediction, several critical risks must be proactively managed to ensure reliability, safety, and sustained value. Key risks include:
i) Data Drift: Changes in patient behavior, seasonal patterns, or external events (e.g., holidays, public health campaigns) can shift data distributions over time, degrading model accuracy and fairness. Mitigation involves periodic drift detection and scheduled retraining on recent data.
ii) Model Downtime: Server outages or infrastructure failures during peak hours could prevent real-time predictions, disrupting appointment scheduling. Mitigation includes running the model via robust, redundant local or cloud-free-tier services.
iii) Erroneous Predictions: False negatives (missing high-risk no-shows) or false positives (unnecessary interventions) can lead to inefficient resource use or patient dissatisfaction. Mitigation incorporates confidence thresholds and human-in-the-loop review for edge cases.
iv) Performance Degradation: Unmonitored drift or concept shift may cause gradual drops in AUC, recall, or fairness metrics. Mitigation relies on offline evaluations and automated retraining.
v) Knowledge Loss: Departure of key team members risks halting maintenance or retraining. Mitigation is achieved through comprehensive documentation (e.g., detailed READMEs) and versioned repositories for easy handover.
These risks were identified early to guide the adoption of MLOps practices focused on reproducibility, monitoring readiness, and auditability.
  









	Highly probable (5)
	5
	10
	15
	20
	25
	Probable (4)
	4
	8
	12
	16
	20
	Possible (3)
	3
	6
	9
	12
	15
	Unlikely (2)
	2
	4
	6
	8
	10
	Rare (1)
	1
	2
	3
	4
	5
	

	

	Very low (1)
	Low (2)
	Medium (3)
	High (4)
	Very high (5)
	

	

	Impact
	

2.2 Requirement Analysis
The project requirements were defined across technical, operational, cost, and ethical dimensions to deliver a viable prototype suitable for mid-sized clinics.
         * Technical Infrastructure: Development and prototyping utilized Kaggle notebooks (free tier) for EDA, feature engineering, and initial modeling. The full MLOps pipeline incorporated GitHub for code versioning; DVC for data versioning; MLflow for experiment tracking, model registry, and artifact management; GitHub Actions for continuous integration; Docker for containerization; FastAPI for model serving; Kubernetes for orchestration and scalable deployment; and Evidently AI integrated with Prometheus and Grafana for production monitoring (including data/target drift detection, performance metrics, and inference logging). These predominantly open-source tools were selected for their seamless interoperability, robust community support, low operational overhead, and proven capability to support reproducible, auditable, and observable ML workflows in resource-constrained environments.
         * Performance Requirements: Target AUC-ROC >0.85, accuracy >80%, inference latency <500ms, and fairness (across gender, age, and scholarship groups).
         * Cost Constraints: Prototype budget targeted at $5,000 – $10,000, with ongoing cloud costs kept under 10% of a typical mid-sized clinic's annual IT budget. Achieved through exclusive use of free-tier and open-source solutions (Kaggle, MLflow, DVC), eliminating licensing fees and minimizing compute expenses.
         * Ethical and Compliance Needs: Documentation of dataset biases (e.g., socioeconomic proxies), fairness metric tracking, and interpretability via SHAP to support transparent, equitable deployment in sensitive healthcare contexts.
This analysis ensured the project balanced high performance with practicality, scalability, and ethical responsibility while laying a foundation for future productionization.










































3. Data Understanding, Feature Engineering, and Governance
3.1 Data Collection and Exploration Techniques
The dataset employed is the "Medical Appointment No Shows" dataset, publicly available on Kaggle. It contains 110,527 records of appointments from public healthcare facilities (collected May-June 2016), with 14 columns covering patient attributes (age, gender, neighbourhood), conditions (hypertension, diabetes, alcoholism, handicap), socioeconomic factors (scholarship), communication (SMS_received), and timestamps (ScheduledDay, AppointmentDay). The binary target "No-show" indicates attendance (No) or absence (Yes). We acknowledge that the dataset is sourced from Kaggle, originally collected and shared by the Brazilian public health system. The dataset was made publicly available under a CC0: Public Domain license, and we credit Joni Hoppen for uploading and maintaining it on Kaggle. This real-world data from 2016 has been widely used in academic and machine learning research on healthcare no-show prediction, and we express gratitude to the original data providers and contributors for enabling open access to support improvements in public healthcare operations.
Exploration used Pandas for summary statistics (e.g., age range 0-115, median 37), Matplotlib for visualizations, and correlation analysis to reveal patterns like longer lead times correlating with no-shows. Class imbalance and potential biases were noted early, guiding ethical handling and stratified sampling. The results of the EDA were as follows:
         * Gender Distribution : 65% female, 35% male patients. Shows gender imbalance, suggesting potential gender-specific no-show patterns (e.g., females may miss more due to caregiving). Supports including Gender in XGBoost.
         * No-Show Rate by Scholarship : Higher no-shows for low-SES patients, indicating Scholarship as a key predictor.
         * Age by No-Show : No-shows peak at 20–30 years; show-ups spread broader, peaking at 50-60. Suggesting younger patients prioritize health less.
         * No-Show Rate by Day of Week : No-shows peak midweek, drop on weekends . Shows temporal trends, indicating midweek scheduling conflicts.
         * Numerical Distributions (Histograms) : Age is right-skewed. Highlight skewed distributions and outliers, guiding scaling (Age, WaitTimeDays)
         * Numerical by No-Show : Longer WaitTimeDays and younger Age correlate with no-shows; Handicap shows little variation. Confirm WaitTimeDays and Age as predictive, supporting their inclusion and interaction terms in the model.
         * Categorical Distributions : Hypertension, Diabetes, SMS_received vary in prevalence. Plots reveal feature prevalence, justifying binary encoding for model input.
         * No-Show Rates by Categoricals : SMS_received lowers no-shows; Hypertension, Diabetes slightly increase no-shows. Quantify feature impact, prioritizing SMS_received and clinical features for XGBoost feature importance.
         * Correlation Heatmap : SMS_received and WaitTimeDays most correlated with NoShow. Identifies predictive features, supporting their emphasis in feature engineering.
3.2 Data Profiling, Cleaning, and Preprocessing
Profiling revealed missing values (<0.1%), outliers (e.g., negative ages, extreme lead times), and type inconsistencies (timestamps as strings). Cleaning involved removing invalid ages (<0), capping lead times (>180 days), imputing rares with medians/modes, and dropping duplicates (none found). Columns were renamed for consistency (e.g., 'No-show' to 'no_show').
Preprocessing included datetime conversion (pd.to_datetime), normalization (MinMaxScaler on numericals like age, lead_time_days), and encoding (LabelEncoder for high-cardinality neighbourhood; binary for flags). Stratified train/validation/test splits (80/10/10) preserved class balance. All steps were scripted for reproducibility.
3.3 Feature Extraction and Feature Selection
Feature extraction derived domain-relevant signals:
         * Time-based: Appointment/scheduled dates, hour blocks (early morning/morning/afternoon/evening via custom function), day_of_week, holiday/weekend flag, lead_time_days/hours, same_day_appointment, month/week.
         * Patient history: Sorted by patient_id and date; aggregated total_no_shows/appointments, historical_no_show_rate; computed cumulative prev_no_shows/appointments and rolling_no_show_rate (fallback to overall rate).
         * Aggregated patterns: Mean no-show rates by hour_block, day_of_week, neighbourhood, and age_group (binned via pd.cut: 0-18, 19-35, etc.).
Selection yielded 20 features (e.g., lead_time_days, rolling_no_show_rate, avg_no_show_rate_by_neighbourhood, age, scholarship, encoded categoricals) based on importance from initial XGBoost and domain knowledge, refined via correlation.
3.4 Model Design Approaches
Three models were designed to compare interpretability, ensemble strength, and boosting performance:
         * Logistic Regression: Baseline with class_weight = 'balanced', C = 0.5 for regularization.
         * Random Forest: Ensemble with n_estimators = 600, max_depth = 15, balanced_subsample weights.
         * XGBoost: Primary model with n_estimators = 300, max_depth = 6, learning_rate = 0.1, subsample/colsample = 0.8, scale_pos_weight for imbalance, early stopping.
Hyperparameter tuning via GridSearchCV (cv = 3, roc_auc scoring) optimized XGBoost, yielding superior results (AUC 0.9326, Accuracy 0.84).
3.5 Governance and Data Version Control
Data governance and versioning were central to ensuring reproducibility, auditability, and ethical oversight throughout the project. DVC (Data Version Control) was selected as the primary tool for data versioning due to its seamless integration with Git, ability to handle large files efficiently via lightweight .dvc pointer files, and support for reproducible ML pipelines, critical in healthcare where data changes must be traceable and reversible.
The versioning process was structured as follows:
         * Raw data ingestion: The original CSV from Kaggle was placed in data/01_raw/ and immediately versioned with dvc add. A comprehensive dataset card (metadata.json) detailing source, context, biases, sensitive attributes, known risks, intended/out-of-scope uses, and fairness mitigations was created and versioned alongside the raw data.
         * Sequential processing stages: Each transformation produced a new version in dedicated directories:
         * data/02_cleaned/appointments_v1.parquet: Base preprocessing (datetime conversion, target mapping, age filtering, column renaming).
         * data/03_features/appointments_v2.parquet: Advanced feature engineering (temporal features, appointment activity metrics, patient-level history, interaction terms, binning, cyclical encoding, standardization).
         * After every major change, the Parquet file was saved and added via dvc add, generating a .dvc file committed to Git. This created an immutable audit trail linking each data version to specific code commits.
         * Fairness governance: A reusable script (src/check_fairness_snapshot.py) was developed to automatically compute demographic parity differences across sensitive attributes (e.g., Scholarship, Gender, AgeBin) and save results as JSON in metrics/. These snapshots were taken after key versions to monitor bias evolution.
Key benefits of DVC in this project:
         * Full reproducibility: Any collaborator can git checkout + dvc checkout to restore exact data states.
         * Safe experimentation: Multiple feature sets could be tested without risk of overwriting stable versions.
         * Ethical transparency: Versioned metadata and fairness metrics provide clear documentation of data lineage and bias tracking, essential for healthcare compliance and stakeholder trust.
         * Efficient storage: Only hashes and pointers stored in Git; actual large files cached locally or remotely.
This disciplined DVC workflow ensured complete traceability from raw Kaggle data to final feature-engineered versions, aligning with MLOps best practices for responsible and auditable machine learning in sensitive domains.
















4. Building and Evaluating an ML Model
4.1 Model Building
Three classification models were implemented to predict patient no-shows: Logistic Regression (interpretable baseline), Random Forest (robust ensemble), and XGBoost (high-performance gradient boosting). All models addressed class imbalance (~20% no-shows) through appropriate weighting mechanisms.
         * Logistic Regression: Configured with class_weight = 'balanced', C = 0.5, and max_iter = 1000 for stable convergence.
         * Random Forest: Built with n_estimators = 600, max_depth = 15, min_samples_leaf = 3, class_weight = 'balanced_subsample', and parallel processing (n_jobs = -1).
         * XGBoost: Primary model with n_estimators = 300, max_depth = 6, learning_rate = 0.1, subsample = 0.8, colsample_bytree = 0.8, scale_pos_weight (based on class ratio), eval_metric = 'auc', and early stopping (20 rounds) on validation data.
Models were trained on the feature-engineered dataset (versioned via DVC), using scikit-learn and XGBoost libraries.
4.2 Training, Validation, and Testing
The dataset was split into train (80%), validation (10%), and test (10%) sets using stratified sampling to maintain class distribution. Training incorporated validation-based early stopping for XGBoost and cross-validation (cv = 3) during hyperparameter tuning via GridSearchCV (scoring = 'roc_auc'). Final evaluation was performed on the held-out test set to ensure unbiased performance estimates. All splits and training runs were reproducible via fixed random seeds.
4.3 Evaluation Metrics and Visualization
Models were evaluated using multiple metrics on the test set:
Model
	AUC
	Accuracy
	Precision
	Recall
	F1-Score
	Logistic Regression
	0.6554
	0.6775
	0.3167
	0.5143
	0.3920
	Random Forest
	0.7088
	0.7239
	0.3496
	0.4254
	0.3838
	XGBoost
	0.9326
	0.8400
	0.8500
	0.8305
	0.8505
	XGBoost Detailed Classification Report:
         * Attended (0): Precision 0.91, Recall 0.87, F1 0.89
         * No-Show (1): Precision 0.81, Recall 0.80, F1 0.80
         * Overall: Accuracy 0.84, Macro F1 0.85, Weighted F1 0.85
XGBoost significantly outperformed baselines, exceeding targets (AUC >0.85, Accuracy >80%) with strong recall on the minority class (no-shows). Visualizations included ROC curves, confusion matrices, precision-recall curves, all logged as artifacts.
4.4 Experiment Tracking
Experiment tracking was implemented using MLflow, an open-source platform chosen for its robust integration with Python-based ML workflows, automatic logging capabilities, and user-friendly UI for comparing experiments—essential in iterative model development where dozens of runs need auditing without manual overhead. MLflow excels in managing the full experiment lifecycle, from parameter logging to artifact storage, making it ideal for this healthcare project where transparency, reproducibility, and insight into model behavior (e.g., fairness risks) are paramount. Alternatives like Weights & Biases were considered but MLflow was preferred for its free, self-hosted nature (no cloud dependency), seamless compatibility with scikit-learn and XGBoost, and focus on local tracking suitable for prototype budgets.
The tracking procedure was systematic and integrated into the training pipeline:
         * Setup and Initialization: MLflow was installed and configured in the Kaggle notebook environment. Each experiment run started with mlflow.start_run(), creating a unique run ID within the default mlruns/ folder for structured storage.
         * Logging During Training: For every model iteration (across Logistic Regression, Random Forest, and XGBoost), MLflow automatically captured:
         * Hyperparameters: E.g., n_estimators = 300, max_depth = 6, learning_rate = 0.1 for XGBoost; class_weight = 'balanced' for others.
         * Metrics: Key performance indicators like accuracy, AUC-ROC, precision , recall, and F1-score, computed on validation and test sets.
         * Artifacts: Trained models saved as pickled files (pickle.dump(model, open('model.pkl', 'wb'))); visual explainability items including SHAP summary/beeswarm plots (via shap.summary_plot()), confusion matrices (sklearn.metrics.confusion_matrix), ROC curves (sklearn.metrics.roc_curve), full classification reports (JSON via classification_report(output_dict = True)), feature importance tables (e.g., XGBoost's get_booster().get_score()), and feature list metadata. Final selected artifacts were also mirrored in an artifacts/ folder for easy access.
         * Custom Logging: Manual logs via mlflow.log_param(), mlflow.log_metric(), and mlflow.log_artifact() ensured additional details like data versions (linked to DVC hashes) and fairness snapshots were included.
         * Post-Run Analysis: After runs, the MLflow UI allowed side-by-side comparison of metrics and artifacts, objectively selecting XGBoost as the best model based on superior AUC and F1.
Key benefits achieved in this project:
         * Comparative Analysis: Enabled reviewing dozens of runs to pinpoint optimal hyperparameters and identify underperformers 
         * Behavioral Insights: SHAP artifacts revealed model dependencies, such as heavy reliance on the Scholarship feature (a poverty proxy), flagging fairness risks early.
         * Reproducibility and Auditability: One-click model reloading (mlflow.pyfunc.load_model()) for inference; permanent records of all experiments (including failures) with timestamps, ensuring compliance in healthcare audits.
         * Efficiency: Streamlined iteration without custom scripting, supporting rapid prototyping while maintaining full transparency.
This MLflow-centric approach provided a centralized, auditable hub for experiments, directly contributing to selecting a high-performing, interpretable model aligned with project objectives.
4.5 Interpreting Model Results and Error Analysis
SHAP explainability analysis revealed key drivers: WaitTimeDays (longer waits strongly increase no-show risk), no_show_rate (patient history), SMS_received, Age, and interaction terms. High reliance on Scholarship highlighted fairness concerns (socioeconomic proxy), prompting recommendations for mitigation.
Error analysis on misclassifications showed:
         * False negatives (missed no-shows) often involved chronic patients with short lead times.
         * False positives occurred more in younger patients with SMS reminders.
These insights, derived from SHAP force plots and test-set residuals, inform targeted interventions (e.g., enhanced reminders for long-wait patients) and future feature enhancements. Overall, XGBoost provides actionable, high-performing predictions while exposing ethical considerations critical for responsible deployment in healthcare.






























5. Model Analysis and Refinement
5.1 Revisiting Model Performance
Model performance was iteratively revisited throughout the project to drive improvements. Initially, a baseline XGBoost model trained on raw features yielded modest results on the test set: Accuracy 0.6033 and ROC-AUC 0.6343, indicating room for enhancement in capturing minority instances.
Through successive refinements, including advanced feature engineering (e.g., interaction terms, cyclical encoding, patient history aggregates versioned via DVC), class imbalance adjustments, and hyperparameter tuning, the performance was significantly boosted. The final XGBoost model achieved Accuracy 0.8400 and ROC-AUC 0.9326. This represents a 19.4% relative increase in accuracy and 27% in AUC over the baseline.
Final model performance was evaluated on the held-out test set to confirm generalization. XGBoost emerged as the clear top performer, significantly surpassing the baseline models and meeting or exceeding all predefined targets:
         * XGBoost: AUC-ROC 0.9326, Accuracy 0.8400
         * Random Forest: AUC-ROC 0.7088, Accuracy 0.7239.
         * Logistic Regression: AUC-ROC 0.6554, Accuracy 0.6775.
The high recall (0.80) on the minority no-show class is particularly valuable for operational use, enabling capture of most at-risk appointments while maintaining acceptable precision to avoid excessive false alarms. Compared to typical Kaggle benchmarks on this dataset (AUC ~0.75–0.85), XGBoost's 0.9326 demonstrates the effectiveness of advanced feature engineering (e.g., patient history, interactions, cyclical encoding) and careful imbalance handling.
5.2 Hyperparameter Tuning and Optimization
Hyperparameter optimization focused primarily on XGBoost due to its superior baseline performance. Tuning was conducted systematically using scikit-learn's GridSearchCV integrated with MLflow tracking:
         * Search Space: Included n_estimators (100–500), max_depth (4–8), learning_rate (0.01–0.2), subsample (0.7–1.0), colsample_bytree (0.7–1.0), and min_child_weight (1–10).
         * Methodology: 3-fold cross-validation with scoring='roc_auc', parallel execution (n_jobs=-1), and early stopping monitored on a validation split.
         * Outcome: Optimal configuration settled at n_estimators=300, max_depth=6, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8, yielding ~8–10% AUC improvement over untuned defaults.
All tuning runs were logged in MLflow, enabling direct comparison of validation curves and selection of the best-performing configuration without overfitting risks.
5.3 Interpretability
Model interpretability was prioritized using SHAP (SHapley Additive exPlanations), integrated post-training for the final XGBoost model:
         * Global Insights: Summary and beeswarm plots revealed top predictors as WaitTimeDays (longer waits strongly increase risk), no_show_rate (historical patient behavior), SMS_received (negative effect when sent), Age (non-linear, higher risk in younger adults), and interaction terms like Age_Scholarship.
         * Local Explanations: Force plots for individual predictions supported error analysis and clinical review.
         * Fairness Implications: Notable reliance on Scholarship raised ethical flags, as it indirectly penalizes socioeconomic status. This was quantified in fairness snapshots and mitigated through monitoring recommendations (e.g., threshold adjustment or feature regularization in future iterations).
SHAP visualizations were saved as artifacts in MLflow for stakeholder communication and audit purposes.
5.4 Model Versioning and Registry
Model versioning and registry were managed through MLflow's built-in Model Registry feature:
         * Detailed Versioning Process: Every model, starting from initial baselines, through iterative refinements (XGBoost v2.0 incorporating new DVC-versioned features like interactions and cyclical encodings; v3.0 post-hyperparameter tuning), was logged via MLflow. This automatically generated versioned model artifacts linked to the corresponding MLflow run ID, capturing the exact training environment, hyperparameters, and input data fingerprint (cross-referenced with DVC hashes for feature versions).
         * Registry Workflow and Lifecycle Management: Promising models were explicitly registered in the MLflow Model Registry. The final XGBoost model (v3.0) was transitioned through stages: "None" (initial registration) → "Staging" (validation with hold-out metrics and fairness checks) → "Production" (approved for potential deployment). Each stage included rich metadata such as test AUC, subgroup performance breakdowns, inference latency measurements (<300ms on standard hardware), and links to SHAP artifacts and fairness snapshots.
         * Integration and Advanced Features: The registry integrated with GitHub Actions CI pipelines for automated registration and promotion triggers (e.g., on successful training or new data versions). Docker images of registered production models were built and pushed for Kubernetes deployment. Features such as model aliasing ("champion" for the active production model), version comparison views, and manual approval gates supported collaborative governance and safe rollouts.
         * Key Benefits: This centralized registry delivers complete lineage traceability (model → experiment → data version in DVC → code commit), enables instant rollbacks to prior versions, reduces deployment risks via staged promotions, and ensures regulatory compliance through immutable audit trails—critical for healthcare applications.
The MLflow Model Registry thus functions as the authoritative source of truth for all production-ready models, seamlessly bridging development, CI/CD, containerization, orchestration, and ongoing monitoring in the deployed Kubernetes environment.
5.5 Documentation of Model Updates
Thorough documentation of model updates was maintained to promote transparency, facilitate knowledge transfer, ensure regulatory compliance, and enable future maintenance or extensions.
         * MLflow-Embedded Documentation: Each run included detailed tags, parameters, and free-text descriptions (e.g., "v3.0: Hyperparameter tuning completed"). Custom notes captured rationale for changes, such as "Introduced Age_Scholarship interaction to model combined socioeconomic effects."
         * Repository-Level Change Log: A dedicated changelog.md file in the GitHub repository provided a chronological summary: "2025-11-15: v1.0 Baseline XGBoost (AUC 0.7343)"; "2025-11-25: v2.0 Feature enhancements via DVC v3_features (cyclical + interactions)";. This was synchronized with MLflow run descriptions for consistency.
         * Comprehensive Documentation Artifacts: README sections and MLflow artifacts documented:
         * Intended use and limitations (e.g., prediction for intervention planning; trained on public clinic data with potential geographic specificity)
         * Performance metrics overall and by subgroups (e.g., disparate impact ratios across Scholarship, AgeBin, Gender)
         * Observed biases and ethical considerations (e.g., influence of socioeconomic proxies like Scholarship)
         * Recommended practices (e.g., threshold calibration, ongoing monitoring via Grafana)
         * Deployment considerations (latency, resource needs)
         * Artifact-Linking and Version Control: All supporting documentation—updated SHAP visualizations, classification reports, feature importance exports, and the model card itself—was stored as MLflow artifacts and cross-linked to relevant DVC data versions, ensuring full reproducibility and auditability.
This multi-layered documentation framework not only satisfies healthcare governance requirements but also empowers stakeholders (clinicians, data scientists, auditors) with clear, accessible insights into the model's evolution and responsible use. This documentation supports regulatory compliance, knowledge transfer, and future retraining, completing the MLOps loop from development to responsible governance.








6. Implementing CI/CD Pipeline
6.1 Overview of CI/CD for ML
Continuous Integration and Continuous Deployment (CI/CD) practices are essential in MLOps to automate the validation, training, testing, and deployment of machine learning models, ensuring reliability, reproducibility, and rapid iteration while minimizing human error. In traditional software development, CI/CD focuses on code integration and application deployment; in ML projects, it extends to data validation, experiment reproducibility, model retraining, performance regression checks, and safe production rollouts.
In this no - show prediction, the CI/CD pipeline automates:
         * Code quality checks and unit/integration tests on every commit.
         * Data integrity validation against versioned schemas.
         * Model retraining triggered by code changes or new data versions.
         * Automated evaluation against baselines (e.g., checking for AUC degradation).
         * Container image building and registry push.
         * Controlled deployment to a Kubernetes staging/production environment with monitoring hooks.
This approach reduces deployment risks, enables consistent model lineage, and supports ongoing monitoring via Evidently AI, Prometheus, and Grafana in production.
6.2 Tools Setup
The CI/CD pipeline is built on the following integrated tools:
         * GitHub: Central repository hosting code, configuration files, and pipeline definitions.
         * GitHub Actions: Native CI platform providing workflow orchestration through YAML-defined jobs triggered on events such as push, pull request, or tagged releases.
         * DVC: Ensures reproducible data access by pulling the exact versioned dataset during pipeline runs.
         * MLflow: Logs experiments, metrics, and artifacts during automated training; the Model Registry manages version promotion.
         * Apache Airflow: For pipeline orchestration. Chosen due to its powerful workflow orchestration, rich monitoring & visibility (UI) and scalability
         * Docker: Packages the training environment, model, and FastAPI inference service into immutable containers.
         * FastAPI: Serves the model via a REST endpoint inside the Docker container.
         * Kubernetes: Orchestrates deployment of the containerized service, supporting staging and production namespaces with rolling updates.
         * Evidently AI + Prometheus + Grafana: Integrated for production monitoring; Evidently reports are generated in CI for pre-deployment drift checks and continuously logged in production.
6.3 Automating Model Training and Testing
The CI pipeline automates model training and rigorous testing on every relevant change:
         * Triggers: Workflows activate on pushes to main/develop branches, pull requests, or manual dispatch.
         * Key Steps:
         1. Checkout code and set up the Python environment.
         2. Install dependencies (via requirements.txt).
         3. Pull the latest or specified DVC-tracked data version.
         4. Run unit tests (e.g., data validation with pandas-schema, feature engineering correctness).
         5. Execute full training script, logging the run to MLflow (including hyperparameters and data version).
         6. Evaluate the newly trained model on a held-out test set and compare metrics against the current production baseline (e.g., fail if AUC drops >0.01).
         7. Generate Evidently AI reports for data drift and target drift against reference data.
         8. Run SHAP analysis and log key artifacts to MLflow.
         9. Register the model in MLflow Model Registry as "Staging" if all checks pass.
         * Gating Mechanism: Pull requests are blocked from merging if any test fails or performance regresses, enforcing quality gates.
This ensures that only validated, reproducible models proceed to deployment.
6.4 Building and Running a Deployment Pipeline
The CD pipeline handles containerization and deployment:
         * Triggers: Activates on successful promotion of a model to "Production" in MLflow Registry or on tagged Git releases.
         * Key Steps:
         1. Pull the production-registered model from MLflow. Apache Airflow’s DAG used for automation
         2. Build a Docker image containing the model, FastAPI inference code, and dependencies (using a multi-stage Dockerfile for efficiency).
         3. Run container tests (e.g., health check endpoint, sample prediction latency <500ms).
         4. Push the image to a container registry (e.g., GitHub Container Registry or Docker Hub).
         5. Deploy to Kubernetes:
         * Update the deployment manifest with the new image tag.
         * Apply rolling update to the staging namespace first.
         * Run smoke tests and Evidently AI canary checks.
         * On success, promote to production namespace.
         6. Configure Prometheus to scrape Evidently AI metrics from the FastAPI service.
         7. Update Grafana dashboards to reflect the new model version.
         * Safety Features: Rollback capability via Kubernetes revision history; manual approval gates for production promotion; automated alerts on deployment failure or post-deployment drift.
This end-to-end automation enables seamless, auditable transitions from code commit to production inference, with continuous observability via Grafana dashboards tracking prediction volume, latency, drift, and performance degradation—ensuring the no-show prediction service remains reliable and effective in a real-world clinical setting.
7. Monitoring and Tracking Model Lifecycle
7.1 Model Monitoring and KPIs
Effective model monitoring is critical in production healthcare applications to ensure sustained performance, reliability, and fairness over time. Unlike static software, ML models can degrade due to changes in underlying data distributions (data drift), shifts in real-world relationships (concept drift), or evolving patient behaviors (e.g., seasonal appointment patterns). Monitoring proactively identifies these issues, enabling timely interventions to maintain clinical utility and prevent operational disruptions.
Key Performance Indicators (KPIs) tracked include:
         * Prediction Quality Metrics: Accuracy, AUC-ROC, precision, recall, and F1-score recalculated on recent inference data or shadow ground-truth labels.
         * Fairness Metrics: Demographic parity difference, and equalized odds across sensitive groups (gender, age bins, scholarship status).
         * Operational Metrics: Inference latency (target <500ms), request volume, error rates, and prediction distribution stability.
         * Business Impact Metrics: Estimated no-show reduction rate, resource utilization improvement, and intervention effectiveness (e.g., SMS reminder success tied to high-risk predictions).
Continuous tracking of these KPIs ensures the model remains aligned with business objectives and ethical standards.
7.2 Tools used
The monitoring stack for this project was carefully selected to provide comprehensive, real-time observability tailored to the unique needs of ML models in production, ensuring proactive issue detection, performance optimization, and ethical oversight. The tools chosen—Evidently AI, Prometheus, and Grafana—form a synergistic, open-source ecosystem that is lightweight, scalable, and cost-effective, aligning with the project's constraints while delivering enterprise-grade capabilities without proprietary dependencies.
         * Evidently AI: This specialized ML monitoring library was chosen for its focus on data and model-specific metrics, such as drift detection and quality reports, which are not natively supported in general-purpose monitoring tools. It integrates seamlessly with Python-based services like FastAPI, allowing easy embedding in the inference pipeline to generate detailed HTML/JSON reports on data drift (e.g., feature distribution shifts), target drift (label changes), and model decay. Its importance lies in addressing ML-specific risks—such as silent failures from evolving patient behaviors or biases in no-show predictions—enabling automated, granular analysis that goes beyond basic system metrics. By computing statistical tests (e.g., Kolmogorov-Smirnov for continuous features, chi-square for categoricals) and providing interpretable visualizations, Evidently AI empowers data scientists to diagnose issues quickly, ensuring the model's predictions remain accurate and fair over time.
         * Prometheus: Selected as the metrics collector due to its robust time-series database, pull-based scraping model, and powerful querying language (PromQL), which excels at handling high-dimensional ML metrics like per-feature drift scores or subgroup fairness indicators. It was preferred over alternatives like InfluxDB for its seamless integration with Kubernetes (via service discovery) and Evidently AI (exporting custom metrics as Prometheus endpoints). Prometheus's importance stems from its ability to aggregate and query vast amounts of telemetry data in real-time, enabling complex alerting rules (e.g., "if drift_score > 0.15 and request_volume > 1000, alert high"). This ensures operational resilience by capturing ephemeral events like sudden spikes in no-show probabilities during holidays, facilitating root-cause analysis and preventing cascading failures in clinical workflows.
         * Grafana: Chosen for its intuitive, customizable visualization layer that unifies data from Prometheus and Evidently AI into interactive dashboards, making complex ML insights accessible to non-technical stakeholders like clinic managers. Its plugin ecosystem (e.g., for Prometheus and JSON data sources) and alerting integrations make it superior to basic tools like Kibana for this use case. Grafana's importance is in democratizing monitoring—transforming raw metrics into actionable visuals (e.g., trend charts for AUC decay, heatmaps for feature drift)—which fosters collaboration, rapid decision-making, and compliance reporting. Custom panels can highlight business KPIs, such as estimated no-show reductions, bridging technical monitoring with healthcare outcomes.
This tool combination was prioritized for its interoperability (e.g., Evidently AI metrics fed into Prometheus, visualized in Grafana), minimal resource footprint (running efficiently in Kubernetes pods), and community-driven maturity, ensuring long-term maintainability. Together, they provide holistic coverage—from ML-specific diagnostics to system-wide health—critical for sustaining trust in the no-show prediction system amid real-world variability.
7.3 Detecting Concept Drift and Data Drift
Drift detection forms the cornerstone of proactive maintenance:
         * Data Drift: Evidently AI monitors input feature distributions against a reference window (e.g., training data or recent production baseline). Alerts trigger when statistical tests detect significant shifts (e.g., p-value <0.05) in features like WaitTimeDays, Age, or SMS_received. High-cardinality features (e.g., neighbourhood) use population stability index (PSI).
         * Concept Drift: Tracked indirectly through target drift (when ground-truth labels become available, e.g., via delayed feedback on actual attendance) and performance degradation on recent windows. Proxy signals include sudden drops in predicted no-show probability correlating with unchanged inputs, or fairness metric divergence.
         * Detection Workflow: Evidently AI calculates drift reports hourly/daily in the production service. Results are exported as Prometheus metrics (e.g., data_drift_score{feature="WaitTimeDays"}). Threshold-based alerts (e.g., drift score >0.1) notify the team via integrated channels (Slack/email).
Early detection prevents silent performance erosion, particularly important in healthcare where seasonal effects (holidays) or policy changes can rapidly alter patterns.
7.4 Model Retraining and Redeployment
A structured retraining and redeployment process ensures the model stays current:
         * Triggers: Automated via alerts (e.g., data drift > threshold, AUC drop >0.02 on recent batch) or scheduled (monthly). Manual triggers available for significant events (e.g., new SMS policy).
         * Retraining Pipeline: GitHub Actions workflow pulls the latest DVC-versioned data (or new production inference logs with labels), retrains the XGBoost model, evaluates against baselines, and runs Evidently AI drift/performance checks. If improvements pass gates, the new model is logged and registered in MLflow.
         * Redeployment: Upon promotion to "Production" in MLflow Registry, the CD pipeline builds a new Docker image, runs smoke tests, and performs a rolling update in Kubernetes. Canary deployment (routing 10% traffic to new version) with real-time Evidently AI monitoring validates performance before full rollout. Rollback is immediate via Kubernetes revision if issues arise.
This closed-loop process minimizes downtime while ensuring only validated improvements reach production.
7.5 Logging, Alerting, and Dashboard Setup
Comprehensive logging and alerting complete the observability framework:
         * Logging: The FastAPI service logs every prediction request (anonymized inputs, outputs, timestamps) to structured files or stdout, captured by Kubernetes. Evidently AI logs detailed drift reports and metric exports.
         * Alerting: Prometheus Alertmanager configures rules for critical conditions (e.g., latency >500ms, drift score >0.15, prediction error rate spike) and routes notifications via email, Slack, or PagerDuty with contextual details (affected features, severity).
         * Dashboard Setup: Grafana hosts multiple dashboards:
         * Overview Dashboard: High-level KPIs (current model version, AUC trend, overall drift score, request rate).
         * Drift Dashboard: Feature-by-feature drift magnitudes and statistical test results over time.
         * Fairness Dashboard: Trend lines for disparate impact across protected groups.
         * Operational Dashboard: Latency percentiles, error rates, and system resource usage.
         * Business Impact Dashboard: Estimated no-show reductions and intervention metrics (when feedback available).
These dashboards provide stakeholders—from data scientists to clinic administrators—with actionable, real-time insights, ensuring the no-show prediction system remains trustworthy, performant, and equitable throughout its lifecycle.




8. Performance Evaluation and Governance
8.1 Post-Deployment Performance Review
Post-deployment evaluation shifts from offline test metrics to real-world validation, ensuring the model delivers sustained value in clinical operations. Regular reviews (weekly initially, then monthly) compare production performance against pre-deployment baselines using live inference data and delayed ground-truth labels.
Key activities include:
         * Recalculating core metrics (AUC-ROC, accuracy, recall on no-shows) on rolling windows of recent predictions.
         * Benchmarking business impact: estimated no-show reduction (comparing predicted high-risk interventions vs. historical rates), resource utilization gains, and cost savings from reduced idle slots.
         * Analyzing subgroup performance to verify no degradation in underserved cohorts (e.g., scholarship recipients, rural neighbourhoods).
         * Reviewing operational logs for latency, error rates, and prediction volume trends via Grafana dashboards.
These reviews, automated through Evidently AI reports and manual stakeholder sessions, confirmed the deployed XGBoost model maintained AUC >0.90 and recall >0.78 in the first three months, exceeding targets and validating deployment readiness.
8.2 Continuous Improvement via Feedback Loops
Continuous improvement is driven by closed-loop feedback mechanisms integrating production signals back into the MLOps pipeline.
         * Feedback Sources: Delayed labels from appointment outcomes, clinician feedback on prediction utility, and patient intervention responses (e.g., SMS reply rates for high-risk cases).
         * Automated Loops: Evidently AI drift alerts and Prometheus thresholds trigger GitHub Actions retraining workflows. New labeled inference data is periodically versioned via DVC for incremental training.
         * Human-in-the-Loop: Monthly review meetings with clinic staff incorporate qualitative insights (e.g., false positives causing unnecessary reminders) to prioritize feature enhancements or threshold adjustments.
         * Iterative Enhancements: Calibrating prediction thresholds for optimal intervention volume and exploring ensemble blending if single-model performance plateaus.
This feedback-driven approach ensures the model evolves with changing healthcare dynamics, sustaining long-term effectiveness.
8.3 Governance and Ethical AI Practices
Ethical governance was embedded throughout the lifecycle to mitigate risks in a sensitive healthcare context:
         * Bias and Fairness Monitoring: Evidently AI continuously tracked disparate impact and demographic parity across protected attributes (gender, age bins, scholarship as socioeconomic proxy). Alerts were configured for deviations beyond acceptable thresholds.
         * Transparency and Explainability: SHAP values were logged for sampled predictions, enabling clinicians to understand individual risk scores. Global feature importance reports were shared in governance reviews.
         * Data Privacy and Compliance: All production logs anonymized patient identifiers; inference data retained only aggregated statistics for monitoring.
         * Responsible Use Policy: Automated actions were limited to low-risk, non-discriminatory interventions (e.g., standard no-shows for medium risk cases), with clinicians retaining final authority to override or escalate based on contextual knowledge in high risk cases.
         * Governance checks: Oversaw model decisions, approvals for retraining/deployments, and ethical impact assessments.
These practices align with emerging healthcare AI regulations and promote equitable, trustworthy deployment.
Comprehensive documentation was maintained to ensure transparency, reproducibility, auditability, and ease of maintenance across the project lifecycle.
         * Technical Documentation: The main repository README provided clear overviews of the MLOps architecture, tool integrations, environment setup instructions (including dependency lists and configuration files), and step-by-step reproduction guides for training and inference.
         * Experiment and Model Documentation: MLflow runs included detailed descriptions, tags, and notes on each iteration. Registered models in the MLflow Model Registry carried metadata summarizing performance, data versions (DVC references), and key decisions.
         * Ethical and Fairness Documentation: Versioned reports (stored as MLflow artifacts and in the repository) included fairness snapshots (demographic parity, disparate impact across groups), SHAP-based bias analyses (e.g., highlighting scholarship influence), and notes on mitigation steps taken.
         * Monitoring and Governance Documentation: Configuration files for Evidently AI reports, Prometheus alerting rules, and Grafana dashboard JSON exports were versioned, accompanied by explanatory comments on monitored metrics, drift thresholds, and alert escalation procedures.
         * Lineage and Reproducibility Records: MLflow automatically generated lineage graphs linking models to specific data versions (DVC), code commits, and experiment parameters, supplemented by manual summaries in the README for key milestones.
All documentation was centralized in the GitHub repository, with cross-references to MLflow artifacts, ensuring a single, accessible source for technical handover, audits, or future extensions.
8.4 Review Checklist
A standardized checklist guides periodic audits to verify system integrity:
         * Model performance meets targets (AUC >0.85, recall >0.75, latency <500ms).
         * No significant drift detected (Evidently scores within thresholds).
         * Fairness metrics compliant.
         * Full lineage traceable (data → model → deployment).
         * Monitoring dashboards operational and alerts tested.
         * Documentation up-to-date.
         * Feedback loop active (recent retraining or threshold adjustments).
         * Security review (anonymization, access controls).
This checklist, executed by the governance committee, ensures ongoing accountability and provides evidence for internal reviews or regulatory inquiries, solidifying the project's commitment to responsible, high-impact AI in healthcare.