# Deploy New Model to Production

## Current Situation

Your **best model (v4, AUC 0.9347)** is trained locally but not yet in production on Cloud Run.

The Cloud Run service is designed to automatically load whichever model is in "Production" stage in MLflow, but it needs the MLflow registry to be deployed.

## Deployment Options

### Option 1: GitHub Actions CI/CD (Recommended) ‚úÖ

This will automatically build, test, and deploy your new model:

```bash
# 1. Add and commit all changes
git add .
git commit -m "feat: deploy best model v4 (AUC 0.9347)"

# 2. Push to trigger automatic deployment
git push origin main
```

**What happens:**
- GitHub Actions workflow runs
- Docker image is built with `mlruns/` directory
- Image is pushed to Google Artifact Registry
- Cloud Run service is updated with new image
- New model (v4) is now in production! üéâ

**Time:** ~5-10 minutes

---

### Option 2: Manual Docker Build & Deploy üîß

If you want to deploy immediately without GitHub Actions:

```bash
# 1. Set your GCP project
gcloud config set project ethika-rag-model

# 2. Build and push Docker image with MLflow registry
gcloud builds submit --tag gcr.io/ethika-rag-model/noshow-predictor:v4 .

# 3. Deploy to Cloud Run
gcloud run deploy ethika-rag-model \
  --image gcr.io/ethika-rag-model/noshow-predictor:v4 \
  --platform managed \
  --region asia-south1 \
  --allow-unauthenticated \
  --set-env-vars MLFLOW_TRACKING_URI=file:///app/mlruns,MODEL_NAME=noshow-prediction-model
```

**Time:** ~3-5 minutes

---

### Option 3: Cloud-Based MLflow Server (Enterprise) üè¢

For continuous deployment without rebuilding containers:

1. **Set up MLflow Tracking Server** (e.g., on Cloud Run or GKE)
2. **Use Cloud Storage** for model artifacts
3. **Update environment variables:**
   ```bash
   MLFLOW_TRACKING_URI=https://your-mlflow-server.com
   ```

Then models auto-update without redeployment!

**Time:** 1-2 hours to set up, but zero-downtime updates after

---

## Quick Deploy (Recommended)

Since you have GitHub Actions already configured:

```bash
# Deploy the new model now
git add mlruns/ reports/ src/ MISSION_ACCOMPLISHED.md
git commit -m "feat: production model v4 with 93.47% AUC"
git push origin main

# Monitor deployment
gcloud run services describe ethika-rag-model \
  --region asia-south1 \
  --format "value(status.url)"
```

---

## Verify Deployment

After deployment completes, test the production API:

```bash
# 1. Check service status
curl https://ethika-rag-model-583756314894.asia-south1.run.app/health

# 2. Verify model version
curl https://ethika-rag-model-583756314894.asia-south1.run.app/model-info

# Expected response:
# {
#   "name": "noshow-prediction-model",
#   "version": "4",
#   "stage": "Production"
# }
```

---

## Why This Matters for Grading üéì

This demonstrates:
- ‚úÖ **Automated CI/CD**: Push to deploy
- ‚úÖ **Model Versioning**: MLflow tracks all versions
- ‚úÖ **Zero-Downtime Deployment**: New model replaces old automatically
- ‚úÖ **Production MLOps**: Industry-standard practices

Your instructor can see that models are automatically promoted and deployed based on performance metrics!

---

## Current vs. Future State

**Before Deploy:**
```
Local: Model v4 (AUC 0.9347) ‚úÖ
Cloud: Old model (unknown version) ‚ùå
```

**After Deploy:**
```
Local: Model v4 (AUC 0.9347) ‚úÖ
Cloud: Model v4 (AUC 0.9347) ‚úÖ
```

---

## Next Step

Run this now to deploy:
```bash
git add .
git commit -m "feat: deploy production model v4 (AUC 93.47%)"
git push origin main
```

Then check GitHub Actions tab to watch the deployment! üöÄ
